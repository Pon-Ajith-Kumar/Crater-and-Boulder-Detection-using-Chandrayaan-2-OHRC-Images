# ğŸ›°ï¸ Crater and Boulder Detection using Chandrayaan 2 OHRC using U-Net Architecture

A complete deep learning pipeline for automatic mask generation and semantic segmentation of OHRC satellite imagery using **U-Net + SAM (Segment Anything Model)**.

---

## ğŸ“Œ Project Overview

This project builds an **end-to-end segmentation system** for high-resolution OHRC images.
The pipeline starts from raw compressed data and ends with **pixel-wise segmentation predictions and visual overlays**.

### ğŸ”¥ Key Highlights

* ğŸ“¦ Automatic dataset extraction and organization
* âœ‚ï¸ Image tiling into square patches
* ğŸ¤– Automatic mask generation using SAM
* ğŸ§  U-Net based semantic segmentation
* ğŸ“Š Dice, IoU, Accuracy evaluation
* ğŸ–¼ï¸ Rich visualization with contour analysis
* âš¡ Mixed precision training + early stopping

---

## ğŸ¯ Problem Statement

Manual annotation of satellite imagery is expensive and time-consuming.

**Goal:**
Build an automated pipeline that:

1. Processes raw OHRC data
2. Generates segmentation masks
3. Trains a deep learning model
4. Produces accurate pixel-level predictions

---

## ğŸ—ï¸ Project Pipeline

```
Raw ZIP Data
   â†“
Extraction & File Moving
   â†“
Image Squaring (1200Ã—1200)
   â†“
Mask Generation (SAM)
   â†“
Train/Val/Test Split
   â†“
U-Net Training
   â†“
Evaluation Metrics
   â†“
Visualization & Contours
```

---

# ğŸ“ Project Structure

```
OHRC-ISRO/
â”‚
â”œâ”€â”€ chandrayaan.png
â”œâ”€â”€ dataset_preparation
â”‚Â Â  â”œâ”€â”€ data_process.ipynb
â”‚Â Â  â”œâ”€â”€ data_process.py          # Image tiling into squares
â”‚Â Â  â”œâ”€â”€ dataset_split.ipynb
â”‚Â Â  â”œâ”€â”€ dataset_split.py         # Train/val/test split
â”‚Â Â  â”œâ”€â”€ dataset_utils.ipynb
â”‚Â Â  â”œâ”€â”€ dataset_utils.py         # Extraction and file organization
â”‚Â Â  â”œâ”€â”€ masks_generator.ipynb
â”‚Â Â  â””â”€â”€ masks_generator.py       # SAM-based mask creation
â”œâ”€â”€ LICENSE
â”œâ”€â”€ main.ipynb
â”œâ”€â”€ main.py                      # Training, evaluation, visualization
â””â”€â”€ best_model.pth               # Saved best model (during model training)
â””â”€â”€ README.md
```

---

# âš™ï¸ Step-by-Step Workflow

---

## 1ï¸âƒ£ Data Extraction and Organization

**File:** `data_utils.py`

### What it does

* Finds ZIP files in raw dataset
* Extracts `.img` files
* Moves them into structured folders

### Key Functions

* `find_and_extract()` â†’ extracts ZIP contents
* `move_files()` â†’ reorganizes dataset
* `list_immediate_subfolders()` â†’ directory discovery

âœ… Ensures raw OHRC data becomes usable.

---

## 2ï¸âƒ£ Image Preprocessing (Square Tiling)

**File:** `data_process.py`

OHRC images are large and rectangular.

### Problem

Deep learning models require fixed-size inputs.

### Solution

* Split images into **1200Ã—1200 square patches**
* Preserve folder structure
* Save tiled images

### Function

```python
create_squares_from_images(input_folder, output_folder)
```

âœ… Produces model-ready image patches.

---

## 3ï¸âƒ£ Automatic Mask Generation using SAM

**File:** `mask_generator.py`

This is the **most important innovation** in your pipeline.


### ğŸš€ Why SAM?

Manual mask labeling is costly.
We use **Meta's Segment Anything Model (SAM)** to auto-generate masks.


### ğŸ”¬ Mask Generation Pipeline

1. Load image
2. Run SAM automatic mask generator
3. Annotate masks
4. Convert to binary mask
5. Save mask


### Key Functions

* `create_mask()`
* `binarize_mask()`
* `process_dataset()`

âœ… Result: Automatic ground truth masks.

---

## 4ï¸âƒ£ Dataset Splitting

**File:** `data_split.py`

### Purpose

Create proper ML splits:

* Train: 80%
* Validation: 10%
* Test: 10%


### Important Features

* Maintains image-mask pairing
* Preserves directory structure
* Validates image width


### Function

```python
split_data(image_files, mask_files, output_folder)
```


âœ… Ensures reliable model evaluation.

---

# ğŸ§  Model Architecture â€” U-Net

**File:** `main.py`


## âœ¨ Why U-Net?

U-Net is ideal for:

* Medical imaging
* Satellite segmentation
* Small datasets


## ğŸ”· Architecture Overview

Encoder â†’ Bottleneck â†’ Decoder with skip connections.

**Blocks used:**

* Conv2D
* BatchNorm
* ReLU
* MaxPool
* ConvTranspose

---

## ğŸ“‰ Loss Function

Combined loss:

```
Loss = 0.5 Ã— BCEWithLogitsLoss + 0.5 Ã— Dice Loss
```

### Why combined?

* BCE â†’ pixel accuracy
* Dice â†’ overlap quality

âœ… Better segmentation performance.

---

# ğŸš€ Training Strategy


## âš¡ Advanced Features Used

* Mixed precision (AMP)
* GradScaler
* ReduceLROnPlateau
* Early stopping
* GPU acceleration


## ğŸ›‘ Early Stopping

Training stops when validation loss stops improving.

From your run:

```
Early stopping triggered!
Best Validation Loss: 0.3419
```

âœ… Prevents overfitting.

---

# ğŸ“Š Evaluation Metrics

The model is evaluated using:

* Dice Score
* IoU Score
* Pixel Accuracy


## ğŸ§ª Your Final Results

| Metric   | Value      |
| -------- | ---------- |
| Dice     | **0.4255** |
| IoU      | **0.2707** |
| Accuracy | **0.9343** |


---

# ğŸ–¼ï¸ Visualization System

The project generates rich visual outputs:

### Views Produced

* Input image
* Ground truth mask
* Overlay (Pred vs GT)
* Contour analysis with circularity

## ğŸ¨ Special Visualization Features

* ğŸ”´ Red = Prediction
* ğŸŸ¢ Green = Ground Truth
* ğŸ”µ Circularity label on contours


# ğŸ’» How to Run the Project
### (Download the dataset from our ISRO website `pradan.issdc.gov.in`)

---

## ğŸ”§ 1. Install Dependencies

```bash
pip install torch torchvision
pip install opencv-python
pip install supervision
pip install segment-anything
pip install tqdm pandas matplotlib
```

---

## ğŸ“‚ 2. Prepare Dataset

Update paths in:

* `data_utils.py`
* `data_process.py`
* `data_split.py`
* `main.py`

---

## ğŸƒ 3. Run Pipeline

### Step 1 â€” Extract data

```bash
python data_utils.py
```

### Step 2 â€” Create square patches

```bash
python data_process.py
```

### Step 3 â€” Generate masks

```bash
python mask_generator.py
```

### Step 4 â€” Split dataset

```bash
python data_split.py
```

### Step 5 â€” Train model

```bash
python main.py
```

---

# ğŸ“ˆ Future Improvements

You can improve performance by:

* ğŸ”¹ Data augmentation
* ğŸ”¹ Attention U-Net
* ğŸ”¹ Dice-only loss tuning
* ğŸ”¹ Larger input resolution
* ğŸ”¹ Better SAM filtering
* ğŸ”¹ Post-processing (morphology)
* ğŸ”¹ ConvLSTM + U-Net

---

# ğŸ¤ Acknowledgements

* ISRO OHRC dataset
* Meta AI â€” Segment Anything Model
* 
---

# ğŸ“Œ Output Sample

![OUTPUT\_PLACEHOLDER](chandrayaan.png)

---

# ğŸ‘¨â€ğŸ’» Author

**Pon Ajith Kumar P**
AI/ML Enthusiast ğŸ‡®ğŸ‡³

---
